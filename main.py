from speechbrain.inference.ASR import EncoderDecoderASR
import torch
import time
import logging
import whisper
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(message)s')

print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("GPU name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"ğŸš€ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device}")

def transcribe_wav(wav_path):
    asr_model = EncoderDecoderASR.from_hparams(
        source="speechbrain/asr-conformer-transformerlm-ksponspeech",
        savedir="pretrained_models/asr-conformer-transformerlm-ksponspeech",
        run_opts={"device": "cuda"}
    )

    logging.info(f"ğŸ¤ ìŒì„± íŒŒì¼ ë¶„ì„ ì‹œì‘: {wav_path}")
    start = time.time()

    txt = asr_model.transcribe_file(wav_path)

    elapsed = time.time() - start
    logging.info(f"âœ… ë¶„ì„ ì™„ë£Œ (ê±¸ë¦° ì‹œê°„: {elapsed:.2f}ì´ˆ)")
    return txt



def transcribe_with_whisper(audio_path: str) -> str:
    # Whisper ëª¨ë¸ ë¡œë“œ (base, small, medium, large ê°€ëŠ¥)
    model = whisper.load_model("medium", device=device)  # ë˜ëŠ” "small", "medium", "large"

    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")

    logging.info(f"ğŸ¤ Whisper ë¶„ì„ ì‹œì‘: {audio_path}")
    start = time.time()

    result = model.transcribe(audio_path, language="ko")

    elapsed = time.time() - start
    logging.info(f"âœ… Whisper ë¶„ì„ ì™„ë£Œ (ê±¸ë¦° ì‹œê°„: {elapsed:.2f}ì´ˆ)")
    return result["text"]  # ìë§‰ í…ìŠ¤íŠ¸ ë°˜í™˜

if __name__ == "__main__":
    wav = "000020.wav"
    # subtitle = transcribe_wav(wav)
    subtitle = transcribe_with_whisper(wav)
    print("ğŸ“œ Subtitle:\n", subtitle)
